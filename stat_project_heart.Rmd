---
title: "Projekt statystyka Wielowymiarowa"
author: "Rafal Pych, Witold Soczek"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tree)
library(randomForest)
library(gbm)
library(MASS) #lda, qda
library(class) #knn
library(leaps)
library(splines)
library(gam)
```

# Analiza zbioru zawieraj??cego dane zwi??zane z chorobami serca:
```{r}
HEART <- read.csv("heart.csv", header = TRUE, na.strings = " ")
HEART <- na.omit(HEART)
print(head(HEART))
summary(HEART)
HEART$target <- factor(ifelse(HEART$target == 1, 0, 1)) # 0,NO - zdrowy, 1,YES - chory
```

```{r}
set.seed(1)
n <- nrow(HEART)
train <- sample(1:n, n / 2)
test <- -train
```

### Klasyfikator z u??yciem drzewa klasyfikacyjnego:
```{r}
heart.dis.tree <- tree(target ~ . - target, data = HEART, subset = train)
tree.class <- predict(heart.dis.tree, newdata = HEART[test,], type = "class")
table(tree.class, HEART$target[test])
mean(tree.class != HEART$target[test])
plot(heart.dis.tree)
text(heart.dis.tree, pretty = 0)
#heart.dis.tree
```

### Klasyfikator z u??yciem przyci??tego(pruning) drzewa klasyfikacyjnego:
```{r}
set.seed(1)
heart.dis.cv <- cv.tree(heart.dis.tree, FUN = prune.misclass)
heart.dis.cv
plot(heart.dis.cv$size, heart.dis.cv$dev, type = "b")

#size.opt <- heart.dis.cv$size[which.min(heart.dis.cv$dev)]
heart.dis.pruned <- prune.misclass(heart.dis.tree, best = 4)
plot(heart.dis.pruned)
text(heart.dis.pruned, pretty = 0)

pruned.class <- predict(heart.dis.pruned, newdata = HEART[test,], 
                        type = "class")
table(pruned.class, HEART$target[test])
mean(pruned.class != HEART$target[test])
```

#### Przyci??te do 4 li??ci drzewo klasyfikacyjne charakteryzuje si?? mniejszym b????dem testowym (~23%) w por??wnaniu do b????du testowego domy??lnego drzewa klasyfikacyjnego(~27%).

### Klasyfikator z u??yciem baggingu
```{r}
heart.dis.bag <- randomForest(target ~ . - target, data = HEART, subset = train, mtry = 13,
                         importance = TRUE)
heart.pred.bag <- predict(heart.dis.bag, newdata = HEART[test,], type = "class")
table(heart.pred.bag, HEART$target[test])
#price.high.bag$confusion
mean(heart.pred.bag != HEART$target[test])
importance(heart.dis.bag)
varImpPlot(heart.dis.bag)
```

### Klasyfikator z u??yciem lasu losowego:
```{r}
heart.dis.rf <- randomForest(target ~ . - target, data = HEART, subset = train,
                         importance = TRUE)
heart.pred.rf <- predict(heart.dis.rf, newdata = HEART[test,], type = "class")
table(heart.pred.rf, HEART$target[test])
mean(heart.pred.rf != HEART$target[test])
importance(heart.dis.rf)
varImpPlot(heart.dis.rf)
```

#### W przypadku u??ycia baggingu(wykorzystanie do podzia????w w??z????w wszystkich predykator??w) i domy??lnej konfiguracji las??w losowych(u??ycie do podza??u w??z????w cz????ci zmiennych z danych) uzyskujemy podobne warto??ci b????d??w testowych w klasyfikacji.
Mo??na zauwa??y?? ??e dla obu metod wa??nymi zmiennymi do podzia??u w??z????w s?? zmienne: cp, thal, exang, sex, thalach

### Klasyfikator z wykorzystaniem boostingu:
```{r}
HEARTB <- data.frame(HEART)
HEARTB$target <- ifelse(HEART$target == 1, 1, 0)

heart.high.boost <- gbm(target ~ . - target, data = HEARTB[train,], distribution = "bernoulli",
                  interaction.depth = 4, n.trees = 1000, shrinkage = 0.01)
heart.pred.boost <- predict(heart.high.boost, newdata = HEARTB[test,], type = "response", n.trees = 1000)
head(heart.pred.boost)
hpred.boost.class <- factor(ifelse(heart.pred.boost < 0.5, 0, 1)) # 0 - zdrowy, 1 - chory
table(hpred.boost.class, HEARTB$target[test])
mean(hpred.boost.class != HEARTB$target[test])

head(HEARTB)
```

#### Metoda Boosting w powy??szej konfiguracji daje podobne wynik b????du testowego do metod baggingu i las??w losowych (~18%).

### Klasyfikacja z u??yciem regresji logistycznej
```{r}
contrasts(HEART$target)
fit.logistic <- glm(target ~ . - target, 
                   family = binomial, data = HEART, subset = train)
summary(fit.logistic)
probs.logistic <- predict(fit.logistic, newdata = HEART[test,], type = "response")
pred.logistic <- factor(ifelse(probs.logistic < 0.5, 0, 1))
table(pred.logistic, HEART$target[test])
mean(pred.logistic != HEART$target[test])
head(HEART)
```

#### Dla regresji logistycznej problem klasyfikacji osoby chorej na serce daje do???? niski b????d testowy(~15%). Ponadto analizuj??c model regresji logistycznej mo??na powiedzie??, ??e zmiennymi statystycznie staotnymi dla modelu s?? zmienne cp, sex, thalach, ca, thal - model w du??ym stopniu zale??y od nich.  

### U??ycie metody LDA do klasyfikacji:

```{r}
fit.lda <- lda(target ~ . - target, 
                   data = HEART, subset = train)
summary(fit.lda)
pred.lda <- predict(fit.lda, newdata = HEART[test,], type = "response")
table(pred.lda$class, HEART$target[test])
mean(pred.lda$class != HEART$target[test])
```

### U??ycie metody QDA do klasyfikacji:

```{r}
fit.qda <- qda(target ~ . - target, 
                   data = HEART, subset = train)
summary(fit.qda)
pred.qda <- predict(fit.qda, newdata = HEART[test,], type = "response")
table(pred.qda$class, HEART$target[test])
mean(pred.qda$class != HEART$target[test])
```

#### Metoda lda ma b????d testowy nieznacznie wi??kszy od regresji logistycznej (tak??e ok. 15%) natomiast qda ma ten b????d na poziomie 20%

### Klasyfikator knn, k = 9:

```{r}

train.set <- HEART[train, !colnames(HEART) %in% c("target")]
test.set <- HEART[test, !colnames(HEART) %in% c("target")]
heart.train <- HEART$target[train]
set.seed(2)
pred.knn.9 <- knn(train.set, test.set, heart.train, k = 9)
pred.knn.9
table(pred.knn.9, HEART$target[test])
mean(pred.knn.9 != HEART$target[test])
```

#### Dla problemu klasyfikacji binarnej metoda knn dla k=9 osi??ga najgorsz?? dok??adno???? (~32%) spo??r??d u??ywanych metod do badanego problemu klasyfikacji. 

## Trzypoziomowa klasyfikacja ci??nienia spoczynkowego krwi - ci??nienie niskie, ??rednie i wysokie - w zale??no??ci od innych cech biometrycznych badanej osoby:
#### Podzia?? na klasy ci??nienia zosta?? przeprowadzony na podstawie najcz????ciej spotykanych warto??ci ci??nienia skurczowego krwi. Dla zdrowego cz??owieka powinno ono nale??e?? do przedzia??u (118, 140) - zosta??o oznaczone tu zmienn?? kategoryczn?? "Medium". Ni??sza warto???? (<118) nale??y do klasy niskiego ci??nienia a warto???? wy??sza ni?? 140 do klasy ci??nienia wysokiego.

```{r}
Press_Level <- factor(ifelse(HEART$trestbps <= 140, ifelse(HEART$trestbps <= 118, "Low", "Medium"), "High"))
HEART3 <- data.frame(HEART, Press_Level)
HEART3 <- HEART3[, !colnames(HEART) %in% c("target")]
head(HEART3)
summary(HEART3)
```

#### Tuple z badanego zbioru maj?? najwi??kszy udzia?? w klasie Medium, co odpowiada typowemu zjawisku w rzeczywisto??ci, gdzie ??rednio najwi??ksza grupa ludzi ma w??a??ciwy poziom ci??nienia krwi.

### Klasyfikator 3-klasowy z u??yciem drzewa klasyfikacyjnego:
```{r}
set.seed(1)
heart3.dis.tree <- tree(Press_Level ~ . - trestbps, data = HEART3, subset = train)
tree.class <- predict(heart3.dis.tree, newdata = HEART3[test,], type = "class")
table(tree.class, HEART3$Press_Level[test])
mean(tree.class != HEART3$Press_Level[test])
plot(heart3.dis.tree)
text(heart3.dis.tree, pretty = 0)
#heart.dis.tree
```

#### Drzewo klasyfikacyjne o 18 li??ciach zapewnia klasyfikacj?? dla 3 klas z b????dem testowym 52%.

### Klasyfikator 3-klasowy z u??yciem przyci??tego(pruning) drzewa klasyfikacyjnego:
```{r}
heart3.dis.cv <- cv.tree(heart3.dis.tree, FUN = prune.misclass)
heart3.dis.cv
plot(heart3.dis.cv$size, heart3.dis.cv$dev, type = "b")

size.opt <- heart.dis.cv$size[which.min(heart.dis.cv$dev)]
heart3.dis.pruned <- prune.misclass(heart3.dis.tree, best = size.opt)
plot(heart3.dis.pruned)
text(heart3.dis.pruned, pretty = 0)

pruned.class <- predict(heart3.dis.pruned, newdata = HEART3[test,], 
                        type = "class")
table(pruned.class, HEART3$Press_Level[test])
mean(pruned.class != HEART3$Press_Level[test])
```

#### Po przyci??ciu drzewa klasyfikacyjnego do poddrzewa z najmniejszym b????dem, zawieraj??cego 14 li??ci, uzyskujemy nieznaczn?? popraw?? jako??ci klasyfikacji, z b????dem testowym 50%.

### Klasyfikator 3-klasowy z u??yciem baggingu:
```{r}
heart3.dis.bag <- randomForest(Press_Level ~ . - trestbps, data = HEART3, subset = train, mtry = 12,
                         importance = TRUE)
heart3.pred.bag <- predict(heart3.dis.bag, newdata = HEART3[test,], type = "class")
table(heart3.pred.bag, HEART3$Press_Level[test])
mean(heart3.pred.bag != HEART3$Press_Level[test])
importance(heart3.dis.bag)
varImpPlot(heart3.dis.bag)
```

### Klasyfikator 3-klasowy z u??yciem lasu losowego:
```{r}
heart3.dis.bag <- randomForest(Press_Level ~ . - trestbps, data = HEART3, subset = train,
                         importance = TRUE)
heart3.pred.bag <- predict(heart3.dis.bag, newdata = HEART3[test,], type = "class")
head(heart3.pred.bag)
table(heart3.pred.bag, HEART3$Press_Level[test])
mean(heart3.pred.bag != HEART3$Press_Level[test])
importance(heart3.dis.bag)
varImpPlot(heart3.dis.bag)
```


### Klasyfikator 3-klasowy z wykorzystaniem boostingu:
```{r}
Press_levelB <- ifelse(HEART$trestbps <= 140, ifelse(HEART$trestbps <= 118, 1, 2), 3)
HEART3B <- data.frame(HEART, Press_levelB)
HEART3B <- HEART3B[, !colnames(HEART3B) %in% c("target")]

heart.high.boost <- gbm(Press_levelB ~ . - trestbps, data = HEART3B[train,], distribution = "multinomial",
                  interaction.depth = 4, n.trees = 500, shrinkage = 0.01)
heart.pred.boost <- predict(heart.high.boost, newdata = HEART3B[test,], type = "response", n.trees = 500)
hpred.boost.class <- apply(heart.pred.boost, 1, which.max)
hpred.boost.class
table(hpred.boost.class, HEART3B$Press_levelB[test])
mean(hpred.boost.class != HEART3B$Press_levelB[test])
head(HEART3B)
```

#### W tym przypadku klasyfikacji bagging uzyska?? najlepsz?? dok??adno???? klasyfikacji z b????dem testowym ~40% w por??wnaniu do metody las??w losowych - b????d testowy to ~42% i  metod?? boostingu gdzie ten b????d wynosi ~44%.

### U??ycie metody LDA do klasyfikacji:

```{r}
fit.lda <- lda(Press_Level ~ . - trestbps, 
                   data = HEART3, subset = train)
summary(fit.lda)
pred.lda <- predict(fit.lda, newdata = HEART3[test,], type = "response")
table(pred.lda$class, HEART3$Press_Level[test])
mean(pred.lda$class != HEART3$Press_Level[test])
```

### U??ycie metody QDA do klasyfikacji:

```{r}
fit.qda <- qda(Press_Level ~ . - trestbps, 
                   data = HEART3, subset = train)
summary(fit.qda)
pred.qda <- predict(fit.qda, newdata = HEART3[test,], type = "response")
table(pred.qda$class, HEART3$Press_Level[test])
mean(pred.qda$class != HEART3$Press_Level[test])
```

#### Metoda lda uzyska??a mniejszy b????d testowy (~42%) w por??wnaniu do metody qda gdzie warto???? tego b????du to ~53%.

### Klasyfikator knn, k = 3:

```{r}

train.set <- HEART3[train, !colnames(HEART3) %in% c("Press_Level")]
test.set <- HEART3[test, !colnames(HEART3) %in% c("Press_Level")]
heart.train <- HEART3$Press_Level[train]
set.seed(2)
pred.knn.3 <- knn(train.set, test.set, heart.train, k = 3)
pred.knn.3
table(pred.knn.3, HEART3$Press_Level[test])
mean(pred.knn.3 != HEART3$Press_Level[test])
```

#### W przypadku metody klasyfikacji z u??yciem knn dla k=3 uzyskujemy do???? dobr?? jako???? klasyfikacji z b????dem testowym ~17%. Jest to najlepsza dok??adno???? klasyfikacji spo??r??d wszystkich badanych metod dla przypadku z 3 klasami ci??nienia krwi.

### O ile metoda knn gorzej radzi??a sobie z problemem klasyfikacji binarnej, a inne metody (czy to bazuj??ce na drzewach decyzyjnych czy na regresji logistycznej) wypada??y wtedy lepiej, to w przypadku gdy wyst??puje potrzeba klasyfikacji dla wiekszej ilo??ci klas zdecydowanie lepiej radzi sobie metoda knn.

## Regresja
### Regresja wielokrotna zmiennej `chol` reprezentującej wartości cholesterolu wśród pacjentów: 

```{r}
lmFit <- lm(chol ~ ., data = HEART)
summary(lmFit)
layout(matrix(c(1,2,3,4),2,2))
plot(lmFit)
```

### Zestawienie prostych regresji liniowych 
```{r}
cols <- names(HEART)[-1]
cols <- cols[!cols %in% c("chol")]
lm.heart <- vector("list", length(cols))

for(i in seq_along(cols)){
    lm.heart[[i]] <- lm(reformulate(cols[i], "chol"), data = HEART)
}
summaries <- lapply(lm.heart, summary)
cols
summaries
```

### Selekcja cech dla modeli nieliniowych za pomocą funkcji `regsubsets()`
```{r}
fit.bs <- regsubsets(chol ~ ., data = HEART, nvmax=12)
fit.bs.summary <- summary(fit.bs)
bic.min <- which.min(fit.bs.summary$bic)
plot(fit.bs.summary$bic, xlab = "Liczba zmiennych", ylab = "BIC", col = "green",
     type = "b", pch = 20)
points(bic.min, fit.bs.summary$bic[bic.min], col = "red", pch = 9)
```

### Selekcja krokowa do przodu i wstecz

Zarówno selekcja krokowa do przodu jak i wstecz dały zbliżone wyniki do powyższych.

### Wybór modelu przy pomocy metody zbioru walidacyjnego
```{r}
fit.bs.v <- regsubsets(chol ~ ., data = HEART[test,], nvmax = 12)
predict.regsubsets <- function(object, newdata, id, ...) {
  model.formula <- as.formula(object$call[[2]])
   mat <- model.matrix(model.formula, newdata)
   coefs <- coef(object, id = id)
   mat[, names(coefs)] %*% coefs
}

pred.error <- function(i, model, subset) {
   pred <- predict(model, HEART[subset,], id = i)
   mean((HEART$chol[subset] - pred)^2)
}
val.errors <- sapply(1:12, pred.error, model = fit.bs.v, subset = test)
val.errors
```
Według powyższego kryterium optymalny model można uzyskać wykorzystując wszystkie zmienne

### Wybór modelu przy pomocy k-krotnej walidacji krzyżowej
```{r}
k <- 10
folds <- sample(1:k, n, replace = TRUE)
validation.errors <- matrix(nrow = k, ncol = 12)
for (j in 1:k) {
   fit.bs.cv <- regsubsets(chol ~ ., data = HEART[folds != j,], nvmax = 12)
   validation.errors[j,] <- 
   sapply(1:12, pred.error, model = fit.bs.cv, subset = (folds == j))
}
 
cv.errors <- apply(validation.errors, 2, mean)
cv.errors
```
Według tej metody optymalny model uzyskamy przy 4 zmiennych

### Regresja wielomianowa
```{r}
attach(HEART)
fit.poly2 <- lm(chol ~ poly(X...age, 2), data = HEART)
fit.poly4 <- lm(chol ~ poly(X...age, 4), data = HEART)
age.lims <- range(X...age)
age.grid <- seq(age.lims[1], age.lims[2])
pred.poly2 <- predict(fit.poly2, list(X...age = age.grid), se.fit = TRUE)
pred.poly4 <- predict(fit.poly4, list(X...age = age.grid), se.fit = TRUE)
se.bands2 <- cbind(pred.poly2$fit + 2 * pred.poly2$se.fit, pred.poly2$fit - 2 * pred.poly2$se.fit)
se.bands4 <- cbind(pred.poly4$fit + 2 * pred.poly4$se.fit, pred.poly4$fit - 2 * pred.poly4$se.fit)
plot(X...age, chol, col = "darkgrey", cex = 0.5, xlim = age.lims)
lines(age.grid, pred.poly2$fit, col = "red", lwd = 2)
lines(age.grid, pred.poly4$fit, col = "blue", lwd = 2)
matlines(age.grid, se.bands2, col = "red", lty = "dashed")
matlines(age.grid, se.bands4, col = "blue", lty = "dashed")
```

### Funkcji schodkowe
```{r}
fit.step4 <- lm(chol ~ cut(X...age, 4), data = HEART)
fit.step8 <- lm(chol ~ cut(X...age, 8), data = HEART)
pred.step4 <- predict(fit.step4, list(X...age = age.grid), se.fit = TRUE)
pred.step8 <- predict(fit.step8, list(X...age = age.grid), se.fit = TRUE)
se.bands4 <- cbind(pred.step4$fit + 2 * pred.step4$se.fit, pred.step4$fit - 2 * pred.step4$se.fit)
se.bands8 <- cbind(pred.step8$fit + 2 * pred.step8$se.fit, pred.step8$fit - 2 * pred.step8$se.fit)

plot(X...age, chol, col = "darkgrey", cex = 0.5, xlim = age.lims)
lines(age.grid, pred.step4$fit, col = "red", lwd = 2)
matlines(age.grid, se.bands4, col = "red", lty = "dashed")

plot(X...age, chol, col = "darkgrey", cex = 0.5, xlim = age.lims)
lines(age.grid, pred.step8$fit, col = "blue", lwd = 2)
matlines(age.grid, se.bands8, col = "blue", lty = "dashed")
```

### Funkcje sklejane
```{r}
fit.bs.knots4 <- lm(chol ~ bs(X...age, df = 4, degree = 4), data = HEART)
fit.bs.knots8 <- lm(chol ~ bs(X...age, df = 8, degree = 4), data = HEART)
pred.bs.knots4 <- predict(fit.bs.knots4, list(X...age = age.grid), se.fit = TRUE)
pred.bs.knots8 <- predict(fit.bs.knots8, list(X...age = age.grid), se.fit = TRUE)

plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.bs.knots4$fit, col = "red", lwd = 2)
lines(age.grid, pred.bs.knots4$fit + 2 * pred.bs.knots4$se.fit, col = "red", lty = "dashed")
lines(age.grid, pred.bs.knots4$fit - 2 * pred.bs.knots4$se.fit, col = "red", lty = "dashed")

plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.bs.knots8$fit, col = "blue", lwd = 2)
lines(age.grid, pred.bs.knots8$fit + 2 * pred.bs.knots8$se.fit, col = "blue", lty = "dashed")
lines(age.grid, pred.bs.knots8$fit - 2 * pred.bs.knots8$se.fit, col = "blue", lty = "dashed")
```

### Naturalne funkcje sklejane
```{r}
fit.ns4 <- lm(chol ~ ns(X...age, df = 4), data = HEART)
fit.ns8 <- lm(chol ~ ns(X...age, df = 8), data = HEART)
pred.ns4 <- predict(fit.ns4, list(X...age = age.grid), se.fit = TRUE)
pred.ns8 <- predict(fit.ns8, list(X...age = age.grid), se.fit = TRUE)

plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.ns4$fit, col = "red", lwd = 2)
lines(age.grid, pred.ns4$fit + 2 * pred.ns4$se.fit, col = "red", lty = "dashed")
lines(age.grid, pred.ns4$fit - 2 * pred.ns4$se.fit, col = "red", lty = "dashed")

plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.ns8$fit, col = "blue", lwd = 2)
lines(age.grid, pred.ns8$fit + 2 * pred.ns8$se.fit, col = "blue", lty = "dashed")
lines(age.grid, pred.ns8$fit - 2 * pred.ns8$se.fit, col = "blue", lty = "dashed")
```


### Regresja lokalna
```{r message=FALSE, warning=FALSE}
# span - stopień wygładzenia
s <- c(0.1,0.3,0.5)
fit.loess.1 <- loess(chol ~ X...age, span = s[1], degree = 1, data = HEART)
fit.loess.2 <- loess(chol ~ X...age, span = s[2], degree = 1, data = HEART)
fit.loess.3 <- loess(chol ~ X...age, span = s[3], degree = 1, data = HEART)
pred.loess.1 <- predict(fit.loess.1, data.frame(X...age = age.grid))
pred.loess.2 <- predict(fit.loess.2, data.frame(X...age = age.grid))
pred.loess.3 <- predict(fit.loess.3, data.frame(X...age = age.grid))
plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.loess.1, col = "red", lwd = 2)
lines(age.grid, pred.loess.2, col = "blue", lwd = 2)
lines(age.grid, pred.loess.3, col = "green", lwd = 2)
legend("topright", legend = paste("smoothness =", s), col = c("red", "blue", "green"), lty = 1, lwd = 2)
```

### Regresja z wykorzystaniem uogólnionych modeli addatywnych
```{r message=FALSE, warning=FALSE}
fit.gam.bf <- gam(chol ~ s(X...age, df = 4), data = HEART)
plot.Gam(fit.gam.bf, col = "red", se = TRUE)
```