---
title: "Projekt statystyka Wielowymiarowa"
author: "Rafal Pych, Witold Soczek"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tree)
library(randomForest)
library(gbm)
library(MASS) #lda, qda
library(class) #knn
library(leaps)
library(splines)
library(gam)
library(glmnet)
```

# Analiza zbioru zawierającego dane związane z chorobami serca:
```{r}
HEART <- read.csv("heart.csv", header = TRUE, na.strings = " ")
HEART <- na.omit(HEART)
print(head(HEART))
summary(HEART)
HEART$target <- factor(ifelse(HEART$target == 1, 0, 1)) # 0,NO - zdrowy, 1,YES - chory
```

```{r}
set.seed(1)
n <- nrow(HEART)
train <- sample(1:n, n / 2)
test <- -train
```

### Klasyfikator z użyciem drzewa klasyfikacyjnego:
```{r}
heart.dis.tree <- tree(target ~ . - target, data = HEART, subset = train)
tree.class <- predict(heart.dis.tree, newdata = HEART[test,], type = "class")
table(tree.class, HEART$target[test])
mean(tree.class != HEART$target[test])
plot(heart.dis.tree)
text(heart.dis.tree, pretty = 0)
#heart.dis.tree
```

### Klasyfikator z użyciem przyciętego(pruning) drzewa klasyfikacyjnego:
```{r}
set.seed(1)
heart.dis.cv <- cv.tree(heart.dis.tree, FUN = prune.misclass)
heart.dis.cv
plot(heart.dis.cv$size, heart.dis.cv$dev, type = "b")

#size.opt <- heart.dis.cv$size[which.min(heart.dis.cv$dev)]
heart.dis.pruned <- prune.misclass(heart.dis.tree, best = 4)
plot(heart.dis.pruned)
text(heart.dis.pruned, pretty = 0)

pruned.class <- predict(heart.dis.pruned, newdata = HEART[test,], 
                        type = "class")
table(pruned.class, HEART$target[test])
mean(pruned.class != HEART$target[test])
```

Przycięte do 4 liści drzewo klasyfikacyjne charakteryzuje się mniejszym błędem testowym (~23%) w porównaniu do błędu testowego domyślnego drzewa klasyfikacyjnego(~27%).

### Klasyfikator z użyciem baggingu
```{r}
heart.dis.bag <- randomForest(target ~ . - target, data = HEART, subset = train, mtry = 13,
                         importance = TRUE)
heart.pred.bag <- predict(heart.dis.bag, newdata = HEART[test,], type = "class")
table(heart.pred.bag, HEART$target[test])
#price.high.bag$confusion
mean(heart.pred.bag != HEART$target[test])
importance(heart.dis.bag)
varImpPlot(heart.dis.bag)
```

### Klasyfikator z użyciem lasu losowego:
```{r}
heart.dis.rf <- randomForest(target ~ . - target, data = HEART, subset = train,
                         importance = TRUE)
heart.pred.rf <- predict(heart.dis.rf, newdata = HEART[test,], type = "class")
table(heart.pred.rf, HEART$target[test])
mean(heart.pred.rf != HEART$target[test])
importance(heart.dis.rf)
varImpPlot(heart.dis.rf)
```

W przypadku użycia baggingu(wykorzystanie do podziałów węzłów wszystkich predykatorów) i domyślnej konfiguracji lasów losowych(użycie do podzału węzłów części zmiennych z danych) uzyskujemy podobne wartości błędów testowych w klasyfikacji.
Można zauważyć że dla obu metod ważnymi zmiennymi do podziału węzłów są zmienne: cp, thal, exang, sex, thalach

### Klasyfikator z wykorzystaniem boostingu:
```{r}
HEARTB <- data.frame(HEART)
HEARTB$target <- ifelse(HEART$target == 1, 1, 0)

heart.high.boost <- gbm(target ~ . - target, data = HEARTB[train,], distribution = "bernoulli",
                  interaction.depth = 4, n.trees = 1000, shrinkage = 0.01)
heart.pred.boost <- predict(heart.high.boost, newdata = HEARTB[test,], type = "response", n.trees = 1000)
head(heart.pred.boost)
hpred.boost.class <- factor(ifelse(heart.pred.boost < 0.5, 0, 1)) # 0 - zdrowy, 1 - chory
table(hpred.boost.class, HEARTB$target[test])
mean(hpred.boost.class != HEARTB$target[test])

head(HEARTB)
```

Metoda Boosting w powyższej konfiguracji daje podobne wynik błędu testowego do metod baggingu i lasów losowych (~18%).

### Klasyfikacja z użyciem regresji logistycznej
```{r}
contrasts(HEART$target)
fit.logistic <- glm(target ~ . - target, 
                   family = binomial, data = HEART, subset = train)
summary(fit.logistic)
probs.logistic <- predict(fit.logistic, newdata = HEART[test,], type = "response")
pred.logistic <- factor(ifelse(probs.logistic < 0.5, 0, 1))
table(pred.logistic, HEART$target[test])
mean(pred.logistic != HEART$target[test])
head(HEART)
```

Dla regresji logistycznej problem klasyfikacji osoby chorej na serce daje dość niski błąd testowy(~15%). Ponadto analizując model regresji logistycznej można powiedzieć, że zmiennymi statystycznie staotnymi dla modelu są zmienne cp, sex, thalach, ca, thal - model w dużym stopniu zależy od nich.  

### Użycie metody LDA do klasyfikacji:

```{r}
fit.lda <- lda(target ~ . - target, 
                   data = HEART, subset = train)
summary(fit.lda)
pred.lda <- predict(fit.lda, newdata = HEART[test,], type = "response")
table(pred.lda$class, HEART$target[test])
mean(pred.lda$class != HEART$target[test])
```

### Użycie metody QDA do klasyfikacji:

```{r}
fit.qda <- qda(target ~ . - target, 
                   data = HEART, subset = train)
summary(fit.qda)
pred.qda <- predict(fit.qda, newdata = HEART[test,], type = "response")
table(pred.qda$class, HEART$target[test])
mean(pred.qda$class != HEART$target[test])
```

Metoda lda ma błąd testowy nieznacznie większy od regresji logistycznej (także ok. 15%) natomiast qda ma ten błąd na poziomie 20%

### Klasyfikator knn, k = 9:

```{r}

train.set <- HEART[train, !colnames(HEART) %in% c("target")]
test.set <- HEART[test, !colnames(HEART) %in% c("target")]
heart.train <- HEART$target[train]
set.seed(2)
pred.knn.9 <- knn(train.set, test.set, heart.train, k = 9)
pred.knn.9
table(pred.knn.9, HEART$target[test])
mean(pred.knn.9 != HEART$target[test])
```

Dla problemu klasyfikacji binarnej metoda knn dla k=9 osiąga najgorszą dokładność (~32%) spośród używanych metod do badanego problemu klasyfikacji. 

### Trzypoziomowa klasyfikacja ciśnienia spoczynkowego krwi - ciśnienie niskie, średnie i wysokie - w zależności od innych cech biometrycznych badanej osoby:
Podział na klasy ciśnienia został przeprowadzony na podstawie najczęściej spotykanych wartości ciśnienia skurczowego krwi. Dla zdrowego człowieka powinno ono należeć do przedziału (118, 140) - zostało oznaczone tu zmienną kategoryczną "Medium". Niższa wartość (<118) należy do klasy niskiego ciśnienia a wartość wyższa niż 140 do klasy ciśnienia wysokiego.


```{r}
Press_Level <- factor(ifelse(HEART$trestbps <= 140, ifelse(HEART$trestbps <= 118, "Low", "Medium"), "High"))
HEART3 <- data.frame(HEART, Press_Level)
HEART3 <- HEART3[, !colnames(HEART) %in% c("target")]
head(HEART3)
summary(HEART3)
```

Tuple z badanego zbioru mają największy udział w klasie `Medium`, co odpowiada typowemu zjawisku w rzeczywistości, gdzie średnio największa grupa ludzi ma właściwy poziom ciśnienia krwi.

### Klasyfikator 3-klasowy z użyciem drzewa klasyfikacyjnego:
```{r}
set.seed(1)
heart3.dis.tree <- tree(Press_Level ~ . - trestbps, data = HEART3, subset = train)
tree.class <- predict(heart3.dis.tree, newdata = HEART3[test,], type = "class")
table(tree.class, HEART3$Press_Level[test])
mean(tree.class != HEART3$Press_Level[test])
plot(heart3.dis.tree)
text(heart3.dis.tree, pretty = 0)
#heart.dis.tree
```

Drzewo klasyfikacyjne o 18 liściach zapewnia klasyfikację dla 3 klas z błędem testowym 52%.

### Klasyfikator 3-klasowy z użyciem przyciętego(pruning) drzewa klasyfikacyjnego:
```{r}
heart3.dis.cv <- cv.tree(heart3.dis.tree, FUN = prune.misclass)
heart3.dis.cv
plot(heart3.dis.cv$size, heart3.dis.cv$dev, type = "b")

size.opt <- heart3.dis.cv$size[which.min(heart3.dis.cv$dev)]
heart3.dis.pruned <- prune.misclass(heart3.dis.tree, best = size.opt)
plot(heart3.dis.pruned)
text(heart3.dis.pruned, pretty = 0)

pruned.class <- predict(heart3.dis.pruned, newdata = HEART3[test,], 
                        type = "class")
table(pruned.class, HEART3$Press_Level[test])
mean(pruned.class != HEART3$Press_Level[test])
```

Po przycięciu drzewa klasyfikacyjnego do poddrzewa z najmniejszym błędem, zawierającego 11 liści, uzyskujemy nieznaczną poprawę jakości klasyfikacji, z błędem testowym 46%.

### Klasyfikator 3-klasowy z użyciem baggingu:
```{r}
heart3.dis.bag <- randomForest(Press_Level ~ . - trestbps, data = HEART3, subset = train, mtry = 12,
                         importance = TRUE)
heart3.pred.bag <- predict(heart3.dis.bag, newdata = HEART3[test,], type = "class")
table(heart3.pred.bag, HEART3$Press_Level[test])
mean(heart3.pred.bag != HEART3$Press_Level[test])
importance(heart3.dis.bag)
varImpPlot(heart3.dis.bag)
```

### Klasyfikator 3-klasowy z użyciem lasu losowego:
```{r}
heart3.dis.bag <- randomForest(Press_Level ~ . - trestbps, data = HEART3, subset = train,
                         importance = TRUE)
heart3.pred.bag <- predict(heart3.dis.bag, newdata = HEART3[test,], type = "class")
head(heart3.pred.bag)
table(heart3.pred.bag, HEART3$Press_Level[test])
mean(heart3.pred.bag != HEART3$Press_Level[test])
importance(heart3.dis.bag)
varImpPlot(heart3.dis.bag)
```


### Klasyfikator 3-klasowy z wykorzystaniem boostingu:
```{r}
Press_levelB <- ifelse(HEART$trestbps <= 140, ifelse(HEART$trestbps <= 118, 1, 2), 3)
HEART3B <- data.frame(HEART, Press_levelB)
HEART3B <- HEART3B[, !colnames(HEART3B) %in% c("target")]

heart.high.boost <- gbm(Press_levelB ~ . - trestbps, data = HEART3B[train,], distribution = "multinomial",
                  interaction.depth = 4, n.trees = 500, shrinkage = 0.01)
heart.pred.boost <- predict(heart.high.boost, newdata = HEART3B[test,], type = "response", n.trees = 500)
hpred.boost.class <- apply(heart.pred.boost, 1, which.max)
hpred.boost.class
table(hpred.boost.class, HEART3B$Press_levelB[test])
mean(hpred.boost.class != HEART3B$Press_levelB[test])
head(HEART3B)
```

W tym przypadku klasyfikacji bagging uzyskał najlepszą dokładność klasyfikacji z błędem testowym ~40% w porównaniu do metody lasów losowych - błąd testowy to ~42% i  metodą boostingu gdzie ten błąd wynosi ~44%.

### Użycie metody LDA do klasyfikacji:

```{r}
fit.lda <- lda(Press_Level ~ . - trestbps, 
                   data = HEART3, subset = train)
summary(fit.lda)
pred.lda <- predict(fit.lda, newdata = HEART3[test,], type = "response")
table(pred.lda$class, HEART3$Press_Level[test])
mean(pred.lda$class != HEART3$Press_Level[test])
```

### Użycie metody QDA do klasyfikacji:

```{r}
fit.qda <- qda(Press_Level ~ . - trestbps, 
                   data = HEART3, subset = train)
summary(fit.qda)
pred.qda <- predict(fit.qda, newdata = HEART3[test,], type = "response")
table(pred.qda$class, HEART3$Press_Level[test])
mean(pred.qda$class != HEART3$Press_Level[test])
```

Metoda lda uzyskała mniejszy błąd testowy (~42%) w porównaniu do metody qda gdzie wartość tego błędu to ~53%.

### Klasyfikator knn, k = 3:

```{r}

train.set <- HEART3[train, !colnames(HEART3) %in% c("Press_Level")]
test.set <- HEART3[test, !colnames(HEART3) %in% c("Press_Level")]
heart.train <- HEART3$Press_Level[train]
set.seed(2)
pred.knn.3 <- knn(train.set, test.set, heart.train, k = 3)
pred.knn.3
table(pred.knn.3, HEART3$Press_Level[test])
mean(pred.knn.3 != HEART3$Press_Level[test])
```

W przypadku metody klasyfikacji z użyciem knn dla k=3 uzyskujemy dość dobrą jakość klasyfikacji z błędem testowym ~17%. Jest to najlepsza dokładność klasyfikacji spośród wszystkich badanych metod dla przypadku z 3 klasami ciśnienia krwi.

O ile metoda knn gorzej radziła sobie z problemem klasyfikacji binarnej, a inne metody (czy to bazujące na drzewach decyzyjnych czy na regresji logistycznej) wypadały wtedy lepiej, to w przypadku gdy występuje potrzeba klasyfikacji dla wiekszej ilości klas zdecydowanie lepiej radzi sobie metoda knn.

## Regresja zmiennej `chol` reprezentującej wartości cholesterolu wśród pacjentów

### Zestawienie prostych regresji liniowych 
```{r}
colnames(HEART)[1] <- "X...age"
attach(HEART)
#head(HEART)
cols <- names(HEART)
cols <- cols[!cols %in% c("chol")]
lm.heart <- vector("list", length(cols))

for(i in seq_along(cols)){
    lm.heart[[i]] <- lm(reformulate(cols[i], "chol"), data = HEART)
}
summaries <- lapply(lm.heart, summary)
cols
summaries
```
Przyglądając się powyższemu zestawieniu zauważyć można że najlepsze dopasowanie modelu uzyskaliśmy dla predyktora związanego z wiekiem (`X...age`)
oraz płcią (`sex`) badanego. 

Wniosek ten wyciągnięty jest na podstawie następujących wartości: 

  podawane w nawisach wartości kolejno dla predyktorów (`X...age` / `sex`)
  
  -> R-squered = (0.045 / 0.039) - największe wartości spośród wszystkich predyktorów, choć mimo to, są to wartości niepokojąco niskie, i dla poprawnego predyktora
    oczekiwalibyśmy wyższych wartości
    
  -> Adj R-Squared = (0.042 / 0.036) - najwyższe wartości spośród wszystkich predykatów
  
  -> F-statistic = (14.4 / 12.27) - najwyższe wartości spośród wszystkich predykatów
  
  -> t-statistic = (0.000179 / 0.00053)
  
  -> Std. Error = (17.71 / 5.19) - w tym przypadku wartość błędu osłabia znaczenie predyktora `X...age` ponieważ jest ona stosunkowo wysoka.

### Regresja wielokrotna 

```{r}
lmFit <- lm(chol ~ ., data = HEART)
summary(lmFit)
layout(matrix(c(1,2,3,4),2,2))
plot(lmFit)
```

to samo wybierając tylko najbardziej znaczące zmienne uzyskane w poprzednim kroku:
```{r}
lmFit <- lm(chol ~ X...age + sex + restecg, data = HEART)
summary(lmFit)
layout(matrix(c(1,2,3,4),2,2))
plot(lmFit)
```

Ograniczając ilość zmiennycj w regresji wielokrotnej uzyskaliśmy poprawę dla metryki `F-Statistic`która w nowym modelu miała wyższą wartość. 
Natomiast wartości błędu kwadratowego zmalały co jest gorszym wynikiem dla nowego modelu.  

Analiza regresji wielokrotnej:

  -> `Residuals vs Fitted` - uzyskaliśmy poziomą wskazującą związek liniowy predyktora względem wyjścia
  
  -> `Normal Q-Q` - otrzymaliśmy wykres podążający za przerywaną linią co oznacza równomierny rozkład błędu (*normally distributed residual*)
  
  -> `Scale-Location` - wksaźnik jednorodności wariancji błędu, uzyskana pozioma linia z równomiernie rozłożonymi wynikami jest dobrą oznaką
  
  -> `Residuals vs Leverage` - określa wartości których wykluczenie lub wzięcie pod uwagę ma znaczący wpływ na linię regresji (w tym przypadku 
  wszystkie wartości znajdując się wewnątrz obszaru zakreślającego dystans Cook'a co jest dobrym wskaźnikiem - brak wartości zaburzących model)
 
### Selekcja cech dla modeli nieliniowych za pomocą funkcji `regsubsets()`
```{r}
fit.bs <- regsubsets(chol ~ ., data = HEART, nvmax=12)
fit.bs.summary <- summary(fit.bs)

bic.min <- which.min(fit.bs.summary$bic) # Schwartz's information criterion
plot(fit.bs.summary$bic, xlab = "Liczba zmiennych", ylab = "Schwartz's information criterion", col = "green", type = "b", pch = 20)
points(bic.min, fit.bs.summary$bic[bic.min], col = "red", pch = 8)

cp.min <- which.min(fit.bs.summary$cp) # Mallows' Cp
plot(fit.bs.summary$cp, xlab = "Liczba zmiennych", ylab = "Mallows' Cp", col = "green", type = "b", pch = 20)
points(cp.min, fit.bs.summary$cp[cp.min], col = "red", pch = 8)

adjr2.max <- which.max(fit.bs.summary$adjr2) # Adjusted r-squared
plot(fit.bs.summary$adjr2, xlab = "Liczba zmiennych", ylab = "Adjusted r-squared", col = "green", type = "b", pch = 20)
points(adjr2.max, fit.bs.summary$adjr2[adjr2.max], col = "red", pch = 8)
```

Jak widzimy każde z kryteriów podało inną ilość cech jako optymalną dla naszego modelu, poniżej estymaty współczynników 
dla kolejno wyznaczonych optymalnych podzbiorów

BIC:
```{r}
coef(fit.bs, id = bic.min)
```

CP:
```{r}
coef(fit.bs, id = cp.min)
```

ADFR2:
```{r}
coef(fit.bs, id = adjr2.max)
```

### Selekcja krokowa do przodu i wstecz

Zarówno selekcja krokowa do przodu jak i wstecz dały zbliżone wyniki do powyższych.

### Wybór modelu przy pomocy metody zbioru walidacyjnego
```{r}
fit.bs.v <- regsubsets(chol ~ ., data = HEART[test,], nvmax = 12)
predict.regsubsets <- function(object, newdata, id, ...) {
  model.formula <- as.formula(object$call[[2]])
   mat <- model.matrix(model.formula, newdata)
   coefs <- coef(object, id = id)
   mat[, names(coefs)] %*% coefs
}

pred.error <- function(i, model, subset) {
   pred <- predict(model, HEART[subset,], id = i)
   mean((HEART$chol[subset] - pred)^2)
}
val.errors <- sapply(1:12, pred.error, model = fit.bs.v, subset = test)
val.errors
```

Według powyższego kryterium optymalny model można uzyskać wykorzystując wszystkie zmienne.

```{r}
fit.bs.v <- regsubsets(chol ~ ., data = HEART[test,], nvmax = 12)
```

### Wybór modelu przy pomocy k-krotnej walidacji krzyżowej
```{r}
k <- 10
folds <- sample(1:k, n, replace = TRUE)
validation.errors <- matrix(nrow = k, ncol = 12)
for (j in 1:k) {
   fit.bs.cv <- regsubsets(chol ~ ., data = HEART[folds != j,], nvmax = 12)
   validation.errors[j,] <- 
   sapply(1:12, pred.error, model = fit.bs.cv, subset = (folds == j))
}
 
cv.errors <- apply(validation.errors, 2, mean)
cv.errors
```

Według tej metody optymalny model uzyskamy przy 4 zmiennych.

## Modele nieliniowe

Poniżej przedstawione są wizualizacje dopasowania modeli nieliniowych (po 2 różne warianty każdy)
### Regresja wielomianowa
```{r}
attach(HEART)
fit.poly2 <- lm(chol ~ poly(X...age, 2), data = HEART)
fit.poly4 <- lm(chol ~ poly(X...age, 4), data = HEART)
age.lims <- range(X...age)
age.grid <- seq(age.lims[1], age.lims[2])
pred.poly2 <- predict(fit.poly2, list(X...age = age.grid), se.fit = TRUE)
pred.poly4 <- predict(fit.poly4, list(X...age = age.grid), se.fit = TRUE)
se.bands2 <- cbind(pred.poly2$fit + 2 * pred.poly2$se.fit, pred.poly2$fit - 2 * pred.poly2$se.fit)
se.bands4 <- cbind(pred.poly4$fit + 2 * pred.poly4$se.fit, pred.poly4$fit - 2 * pred.poly4$se.fit)
plot(X...age, chol, col = "darkgrey", cex = 0.5, xlim = age.lims)
lines(age.grid, pred.poly2$fit, col = "red", lwd = 2)
lines(age.grid, pred.poly4$fit, col = "blue", lwd = 2)
matlines(age.grid, se.bands2, col = "red", lty = "dashed")
matlines(age.grid, se.bands4, col = "blue", lty = "dashed")
```

Zwiększenie stopnia wielomianu nie poprawiło znacząco dopasowania modelu, jedyne zauważalne różnice widzimy
dla wartości skrajnych

### Funkcji schodkowe
```{r}
fit.step4 <- lm(chol ~ cut(X...age, 4), data = HEART)
fit.step8 <- lm(chol ~ cut(X...age, 8), data = HEART)
pred.step4 <- predict(fit.step4, list(X...age = age.grid), se.fit = TRUE)
pred.step8 <- predict(fit.step8, list(X...age = age.grid), se.fit = TRUE)
se.bands4 <- cbind(pred.step4$fit + 2 * pred.step4$se.fit, pred.step4$fit - 2 * pred.step4$se.fit)
se.bands8 <- cbind(pred.step8$fit + 2 * pred.step8$se.fit, pred.step8$fit - 2 * pred.step8$se.fit)

plot(X...age, chol, col = "darkgrey", cex = 0.5, xlim = age.lims)
lines(age.grid, pred.step4$fit, col = "red", lwd = 2)
matlines(age.grid, se.bands4, col = "red", lty = "dashed")

plot(X...age, chol, col = "darkgrey", cex = 0.5, xlim = age.lims)
lines(age.grid, pred.step8$fit, col = "blue", lwd = 2)
matlines(age.grid, se.bands8, col = "blue", lty = "dashed")
```

Zwiększenie ilości punktów załamań (*breaks*) poprawiło dopasowanie modelu

### Funkcje sklejane
```{r}
fit.bs.knots4 <- lm(chol ~ bs(X...age, df = 4, degree = 4), data = HEART)
fit.bs.knots8 <- lm(chol ~ bs(X...age, df = 8, degree = 4), data = HEART)
pred.bs.knots4 <- predict(fit.bs.knots4, list(X...age = age.grid), se.fit = TRUE)
pred.bs.knots8 <- predict(fit.bs.knots8, list(X...age = age.grid), se.fit = TRUE)

plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.bs.knots4$fit, col = "red", lwd = 2)
lines(age.grid, pred.bs.knots4$fit + 2 * pred.bs.knots4$se.fit, col = "red", lty = "dashed")
lines(age.grid, pred.bs.knots4$fit - 2 * pred.bs.knots4$se.fit, col = "red", lty = "dashed")

plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.bs.knots8$fit, col = "blue", lwd = 2)
lines(age.grid, pred.bs.knots8$fit + 2 * pred.bs.knots8$se.fit, col = "blue", lty = "dashed")
lines(age.grid, pred.bs.knots8$fit - 2 * pred.bs.knots8$se.fit, col = "blue", lty = "dashed")
```

Funkcja sklejana wyższego stopnia wydaje się być lepiej dopasowana do większości danych, 
niepokojące mogą być natomiast krańcowe wartości, istnieje tutaj obawa o "nadmierne dopasowania"

### Naturalne funkcje sklejane
```{r}
fit.ns4 <- lm(chol ~ ns(X...age, df = 4), data = HEART)
fit.ns8 <- lm(chol ~ ns(X...age, df = 8), data = HEART)
pred.ns4 <- predict(fit.ns4, list(X...age = age.grid), se.fit = TRUE)
pred.ns8 <- predict(fit.ns8, list(X...age = age.grid), se.fit = TRUE)

plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.ns4$fit, col = "red", lwd = 2)
lines(age.grid, pred.ns4$fit + 2 * pred.ns4$se.fit, col = "red", lty = "dashed")
lines(age.grid, pred.ns4$fit - 2 * pred.ns4$se.fit, col = "red", lty = "dashed")

plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.ns8$fit, col = "blue", lwd = 2)
lines(age.grid, pred.ns8$fit + 2 * pred.ns8$se.fit, col = "blue", lty = "dashed")
lines(age.grid, pred.ns8$fit - 2 * pred.ns8$se.fit, col = "blue", lty = "dashed")
```

Naturalna funkcja sklejana 8 stopnia daje "bezpieczniejsze" dopasowanie dla krańcowych wartości w stosunku do funkcji
uzyskanej w punkcie poprzednim

### Regresja lokalna
```{r message=FALSE, warning=FALSE}
# span - stopień wygładzenia
s <- c(0.1,0.3,0.5)
fit.loess.1 <- loess(chol ~ X...age, span = s[1], degree = 1, data = HEART)
fit.loess.2 <- loess(chol ~ X...age, span = s[2], degree = 1, data = HEART)
fit.loess.3 <- loess(chol ~ X...age, span = s[3], degree = 1, data = HEART)
pred.loess.1 <- predict(fit.loess.1, data.frame(X...age = age.grid))
pred.loess.2 <- predict(fit.loess.2, data.frame(X...age = age.grid))
pred.loess.3 <- predict(fit.loess.3, data.frame(X...age = age.grid))
plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid, pred.loess.1, col = "red", lwd = 2)
lines(age.grid, pred.loess.2, col = "blue", lwd = 2)
lines(age.grid, pred.loess.3, col = "green", lwd = 2)
legend("topright", legend = paste("smoothness =", s), col = c("red", "blue", "green"), lty = 1, lwd = 2)
```

### Regresja z wykorzystaniem uogólnionych modeli addatywnych
```{r message=FALSE, warning=FALSE}
fit.gam.bf <- gam(chol ~ s(X...age, df = 4), data = HEART)
age.grid <- seq(age.lims[1], age.lims[2])
pred.gam <- predict(fit.gam.bf, list(X...age = age.grid), se.fit = TRUE)
plot(X...age, chol, cex = 0.5, col = "darkgrey")
lines(age.grid,pred.gam, col = "red", se = TRUE)
```

### Regresja grzbietowa
```{r}
X <- model.matrix(HEART$chol ~ ., data = HEART)[, -1]
y <- HEART$X...age

lambda.grid <- 10^seq(10, -2, length.out = 100)
fit.ridge <- glmnet(X, y, alpha = 0, lambda = lambda.grid)
set.seed(1)
n <- nrow(X)
train <- sample(1:n, n / 2)
test <- -train
```

dla przykładowego lambda = 5
```{r}
pred.ridge <- predict(fit.ridge, x = X[train,], y = y[train], s = 5, newx = X[test,])
mean((pred.ridge - y[test])^2)
```

dla optymalnego lambda
```{r}
cv.out <- cv.glmnet(X[train,], y[train], alpha = 0)
plot(cv.out)
cv.out$lambda.min # optymalne lambda
fit.ridge <- glmnet(X[train,], y[train], alpha = 0, lambda = lambda.grid, thresh = 1e-12)
pred.ridge <- predict(fit.ridge, s = cv.out$lambda.min, newx = X[test,])
mean((pred.ridge - y[test])^2)
```

### Regresja z wykorzystaniem metody Lasso
```{r}
fit.lasso <- glmnet(X[train,], y[train], alpha = 1)
cv.out <- cv.glmnet(X[train,], y[train], alpha = 1)
plot(cv.out)
cv.out$lambda.min
pred.lasso <- predict(fit.lasso, s = cv.out$lambda.min, newx = X[test,])
mean((pred.lasso - y[test])^2)
```